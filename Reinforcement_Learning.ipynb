{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reinforcement Learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOM32IcF4vjcOOO/hfnMNPk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/koushikpr/Machine-Learning-Prerequisites/blob/Reinforcement-Learning/Reinforcement_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reinforcement Learning: The method by which, the machine learns the process and comes up with a model of its own."
      ],
      "metadata": {
        "id": "RZwnRYos9boq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some Terminologies To Go Ahead\n",
        "\n",
        "1. Environment: Like the platform or the surrounding that our agent will explore\n",
        "2. Agent: An entity that explores the environment.\n",
        "3. State: Tells us the status of the agent in the environment\n",
        "4. Action: Any interaction between the agent and the environment is considered an action.\n",
        "5. Reward: The actions taken by the agent results in rewards(output). Our goal is to maximise the rewards."
      ],
      "metadata": {
        "id": "QYlDwb5f9nvS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q-Learning\n",
        "\n",
        "Q-Learning is a method used learning matrix of action-reward. The matrix is in the form of nxm where, \n",
        "\n",
        "n = no of states\n",
        "\n",
        "m = no of actions\n",
        "\n",
        "This is an integration of Linear Algebra and Truth Tables."
      ],
      "metadata": {
        "id": "JGfGc36K_tN7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Learning the Q-Table\n",
        "\n",
        "Lets look into the finding the values with the help of stages.\n",
        "\n",
        "Stage-1: Assigning 0 as the initial condition for the Q-Table\n",
        "\n",
        "Stage-2: There are 2 ways for learning the values for the Q-Table:\n",
        "\n",
        "a. Randomly picking actions\n",
        "\n",
        "b. Using the current Q-table to predict the next\n",
        "\n",
        "As a beginner we will be looking into assigning random actions and later on we will use these tables to assign the next values. The agent will stop taking actions if it has reached the time limit/completed the goal/reached the end of the environment\n",
        "\n",
        "Stage-3: The formula for updating the Q-Table after each action is as follows:\n",
        "> $ Q[state, action] = Q[state, action] + \\alpha * (reward + \\gamma * max(Q[newState, :]) - Q[state, action]) $\n",
        "\n",
        "- $\\alpha$ stands for the **Learning Rate**\n",
        "\n",
        "- $\\gamma$ stands for the **Discount Factor**\n",
        "\n",
        "\n",
        "Learning rate: $\\alpha$ is a numerical value that depict how much change occurs between any different stages of the Q-Table. Higher value means there is a drastic change between stages in the Table.\n",
        "\n",
        "Discount Factor: $\\gamma$ depicts how much focus is put on the current and future rewards. Higher value means the o/p is considered heavily.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "srl4T0VqB7yc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q-learning Example"
      ],
      "metadata": {
        "id": "tZwy_ctcG5r4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example we will be using openAI which is platform for developers to build machine learning models.\n"
      ],
      "metadata": {
        "id": "EwsC5445G9TK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Importing Open AI Gym"
      ],
      "metadata": {
        "id": "-xmYSlBvHOCe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym"
      ],
      "metadata": {
        "id": "bWROBvYwHmPN"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Loading an environment"
      ],
      "metadata": {
        "id": "dxP_b9JSHlxx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env  = gym.make('FrozenLake-v0')"
      ],
      "metadata": {
        "id": "d09UFRsXHy29"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Few basics interaction commands"
      ],
      "metadata": {
        "id": "5PI9PKYKIp2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(env.observation_space.n)# no of states\n",
        "print(env.action_space.n)# no of actions\n",
        "env.reset()# resets the environment to default stage\n",
        "action  = env.action_space.sample() # gets a random action\n",
        "newstate,reward,done,info = env.step(action) # returns information abt the certain action\n",
        "print(newstate,reward,done,info)\n",
        "env.render() # returns a GUI of the environment"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7h3zwEdLIvGR",
        "outputId": "6eea5110-f091-4792-918e-1111d1ef2c9e"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16\n",
            "4\n",
            "0 0.0 False {'prob': 0.3333333333333333}\n",
            "  (Down)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is our Frozenlake Environment:\n",
        "\n",
        "Our goal in the frozen lake environment is to navigate through the ice without breaking it to reach to the other side.There are \n",
        "\n",
        "16 stages for each square.\n",
        "\n",
        "4 actions we have are (left,right,up,down)\n",
        "\n",
        "4 Types of blocks(frozen,hole,start,goal)"
      ],
      "metadata": {
        "id": "Z-CA8r32KhgS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Building the Q-Table"
      ],
      "metadata": {
        "id": "wDxLuc4XMLHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "env = gym.make('FrozenLake-v0')\n",
        "states = env.observation_space.n\n",
        "actions = env.action_space.n\n",
        "\n",
        "Q=np.zeros((states,actions))#creating a base Q-Table with all zeros\n",
        "Q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WO_XDvcmNEak",
        "outputId": "b8b3f39b-fc62-40c1-f3df-d624912df739"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Assigning Constants"
      ],
      "metadata": {
        "id": "PSHTmqFONrS1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "episodes = 20000 #epochs \n",
        "turns = 100 #max steps\n",
        "\n",
        "learningrate = 0.81\n",
        "gamma = 0.96\n",
        "\n",
        "render = False #this is to see the render\n",
        "e = 0.9\n"
      ],
      "metadata": {
        "id": "JuCljAsyNuF8"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Picking an Action\n",
        "\n",
        "We know that we have 2 ways to pick an action random or learning. here we will define a constant that gives us the probability of selecting a random action or learning from the table"
      ],
      "metadata": {
        "id": "reI4w-W1On3L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "e = 0.9\n",
        "#picking the action based on the random probability\n",
        "if np.random.uniform(0,1)< e:#selecting the action based on uniform Random Variable\n",
        "  action  = env.action_space.sample()#takes a random action from the envoirnment\n",
        "else:\n",
        "  action = np.argmax(Q[state,:])#selects the max argument from the current state(row)"
      ],
      "metadata": {
        "id": "HAi4QCspQC7N"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6: Extraction States and updating Q-Values"
      ],
      "metadata": {
        "id": "hLMXKd2ETHLu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rewards = []\n",
        "\n",
        "for episode in range(episodes):\n",
        "  state = env.reset()#resets the environment\n",
        "  for _ in range(turns):\n",
        "    if render:#to check the render\n",
        "      env.render()\n",
        "\n",
        "    if np.random.uniform(0,1)<e:\n",
        "      action = env.action_space.sample()\n",
        "    else:\n",
        "      action = np.argmax(Q[state,:])\n",
        "\n",
        "    nextstate,reward,done,_ = env.step(action) #returns the values of the current action\n",
        "\n",
        "    Q[state,action] = Q[state,action] + learningrate * (reward + gamma *np.max(Q[newstate,:])-Q[state,action])\n",
        "\n",
        "    state = nextstate#goes to the next stage\n",
        "\n",
        "    if done:#if reached goal\n",
        "      rewards.append(reward)\n",
        "      e -=0.001\n",
        "      break \n",
        "\n",
        "print(Q)\n",
        "avg  = sum(rewards)/len(rewards)\n",
        "print(avg)"
      ],
      "metadata": {
        "id": "tBhZ2hUDTdy4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9438976-8585-43ff-8845-ab0a364f86e1"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.00020944 0.183141   0.15495698]\n",
            " [0.         0.         0.         0.        ]]\n",
            "0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "NRvsaTCGW7bK"
      },
      "execution_count": 42,
      "outputs": []
    }
  ]
}